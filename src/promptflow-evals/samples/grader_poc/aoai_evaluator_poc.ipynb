{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208b8c4f",
   "metadata": {},
   "source": [
    "# AOAI Evaluators POC\n",
    "The goal of this proof of concept is to help us understand how to integrate graders into the evaluation SDK and identify potential gaps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8229d8",
   "metadata": {},
   "source": [
    "## Grader Architecture\n",
    "The diagram is based on the learning from code [here](https://msdata.visualstudio.com/Vienna/_git/azureml-asset?path=/assets/training/evaluation/src/graders/graders.py&version=GBmain).\n",
    "\n",
    "![Grader Arch Image](grader_arch.png)\n",
    "\n",
    "\n",
    "Three major concepts:\n",
    "- *Model Sampler*: take the evaluation input prompt and generate the completion from the users requested deployed model. \n",
    "- *Graders*: compare and score the evaluation completion against the provided evaluation expected outcome. \n",
    "- *Aggregators* â€“ take the results of the grading step and aggregate them over the evaluation job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82be2f",
   "metadata": {},
   "source": [
    "## Grader List\n",
    "\n",
    "| Category               | Name                             | Model Based | Priority | Notes                                                                                                             |\n",
    "|------------------------|----------------------------------|-------------|----------|-------------------------------------------------------------------------------------------------------------------|\n",
    "| **Text Matching**      | StringCheckGrader                | No          |          | Need to check PF team whether they can support formatted string<br> ```lhs```: {`\"type\"`: `\"tstr\"`, `\"value\"`: `\"My name is {item.col2}\"`}<br> ```rhs```: {`\"type\"`: `\"tstr\"`, `\"value\"`: `\"My name is {sample.col2}\"`} |\n",
    "|                        | SetMembershipGrader              | No          |          |                                                                                                                   |\n",
    "|                        | SetComparisionGrader             | No          |          |                                                                                                                   |\n",
    "|                        | StringCountGrader                | No          |          |                                                                                                                   |\n",
    "| **Classification Quality** | ModelGrader                      | Yes         |          | It's built on top of the model sampler. Need to support formatted chat messages. Like our composite evaluator. Similar to what evaluate API does |\n",
    "|                        | MulticlassClassificationGrader   | No          |          |                                                                                                                   |\n",
    "|                        | DiscreteClassificationModelGrader| Yes         |          |                                                                                                                   |\n",
    "| **Text Similarity**    | RougeScoreGrader                 | No          |          |                                                                                                                   |\n",
    "|                        | BleuScoreGrader                  | No          |          |                                                                                                                   |\n",
    "|                        | MeteorScoreGrader                | No          |          |                                                                                                                   |\n",
    "|                        | GleuScoreGrader                  | No          |          |                                                                                                                   |\n",
    "| **Conversational**     | ClosedQAModelGrader              | Yes         |          | Prompt based. Can be implemented using Prompty.                                                                   |\n",
    "|                        | ChatCriteriaModelGrader          | Yes         |          | Prompt based. Can be implemented using Prompty.                                                                   |\n",
    "| **Criteria Based**     | FactualityModelGrader            | Yes         |          | Prompt based. Can be implemented using Prompty.                                                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a3290",
   "metadata": {},
   "source": [
    "## Key Differences Between AOAI Graders and pf-evals Evaluators\n",
    "\n",
    "|                      | **AOAI Graders**                                   | **pf-evals SDK**                                                 |\n",
    "|----------------------|----------------------------------------------------|------------------------------------------------------------------|\n",
    "| **Input/Output**     | Multi-rows                                         | Single row <br> Multi-rows through evaluate API                  |\n",
    "| **Data Access**      | Through Expression Config. Supports three types: <br> 1. Constant Value <br> 2. Column Reference <br> 3. Template String | Similar to column mapping in the evaluation SDK. <br> Most scenarios can be supported through column mapping, but we have a few limitations requiring support from the PF side: <br> 1. Template String <br> ```Question: ${data.question} A: ${data.option_a} B: ${data.option_b} C: ${data.option_c} D: ${data.option_d}``` <br> 2. Column Reference in an array. For example: <br> ```\"element\": \"${data.ground_truth}\", \"set\": [\"A\", \"B\", \"C\", \"D\"]``` |\n",
    "| **Aggregation**      | Per Grader Aggregation <br> Multiple Aggregation Types Supported | Single Aggregation applied to all evaluators <br> Currently only supports mean |\n",
    "| **Model Sampler**    | Generates samples based on formatted chat messages | This concept is not present in pf-evals today, but it functions like a generic target function and can be implemented through prompty. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66741d31",
   "metadata": {},
   "source": [
    "## Graders in Evaluation SDK Demo\n",
    "We have done the following in this POC:\n",
    "\n",
    "1. Migrated three graders (Bleu, String Count and Set Membership) to the evaluation SDK to showcase the primary scenario.\n",
    "2. Implemented ModelSampler based on prompty, with support for formatted chat messages.\n",
    "3. Added support for evaluate API to work with graders.\n",
    "4. Support per evaluator aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b29c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-07 12:50:07 -0700][promptflow][DEBUG] - preparing home directory with default value.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint as print\n",
    "\n",
    "from promptflow.evals.evaluators import BleuScoreEvaluator, SetMembershipEvaluator, StringCountEvaluator\n",
    "from promptflow.evals.evaluators import BleuScoreConfig, SetMembershipConfig, StringCountConfig\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "from promptflow.evals.evaluate import evaluate, ModelSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c443c",
   "metadata": {},
   "source": [
    "### 1. Single line test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b57f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu_score': 0.17141814854755813}\n"
     ]
    }
   ],
   "source": [
    "# BleuScoreEvaluator\n",
    "bleu = BleuScoreEvaluator()\n",
    "score = bleu(\n",
    "    reference=\"this is a book\", hypothesis=\"this is not a book\"\n",
    ")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54c4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'string_count': 3}\n"
     ]
    }
   ],
   "source": [
    "# StringCountGrader\n",
    "string_count = StringCountEvaluator(\n",
    "    StringCountConfig(case_sensitive=False)\n",
    ")\n",
    "score = string_count(\n",
    "    reference=\"A A A\", hypothesis=\"A\"\n",
    ")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d383dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'set_membership': 1}\n"
     ]
    }
   ],
   "source": [
    "# SetMembershipEvaluator\n",
    "membership = SetMembershipEvaluator(\n",
    "    SetMembershipConfig(present_grade=1, absent_grade=0, aggregation_type=\"sum\")\n",
    ")\n",
    "score = membership(\n",
    "    element=\"A\", set=[\"A\", \"B\"]\n",
    ")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13909d25",
   "metadata": {},
   "source": [
    "### Try out ModelSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28517f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample': 'A'}\n"
     ]
    }
   ],
   "source": [
    "# Model Sampler\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    ")\n",
    "\n",
    "sampling_params = {\n",
    "    \"max_tokens\": 1,\n",
    "}\n",
    "\n",
    "# use formatted string in chat messages\n",
    "trajectory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a useful bot. Given a question and some possible answers, reply with the letter of the correct answer.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question: ${data.question} A: ${data.option_a} B: ${data.option_b} C: ${data.option_c} D: ${data.option_d}\"},\n",
    "]\n",
    "sampler = ModelSampler(model_config=model_config, trajectory=trajectory, sampling_params=sampling_params)\n",
    "\n",
    "inputs = {\n",
    "    \"question\": \"What is the capital of France?\",\n",
    "    \"option_a\": \"Paris\",\n",
    "    \"option_b\": \"Berlin\",\n",
    "    \"option_c\": \"London\", \\\n",
    "    \"option_d\": \"Madrid\"\n",
    "}\n",
    "\n",
    "print(sampler(line_data=inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f255f7a",
   "metadata": {},
   "source": [
    "### 3. Batch Run with Evaluate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Run\n",
    "path = \"aoai_evaluator_data.jsonl\"\n",
    "result = evaluate(\n",
    "    data=path,\n",
    "    target=sampler,\n",
    "    evaluators={\n",
    "        \"bleu\": bleu,\n",
    "        \"membership\": membership,\n",
    "        \"string_count\": string_count\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"bleu\": {\"reference\": \"${data.ground_truth}\", \"hypothesis\": \"${target.sample}\"},\n",
    "        \"string_count\": {\"reference\": \"${data.ground_truth}\", \"hypothesis\": \"A\"},\n",
    "        \"membership\": {\"element\": \"${data.ground_truth}\", \"set\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "    },\n",
    "    azure_ai_project = {\n",
    "        \"subscription_id\": \"2d385bf4-0756-4a76-aa95-28bf9ed3b625\",\n",
    "        \"resource_group_name\": \"rg-ninhuai\",\n",
    "        \"project_name\": \"ninhu-3593\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df27c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu.bleu_score_mean': 0.6666666666666666,\n",
      " 'membership.set_membership_sum': 3,\n",
      " 'string_count.string_count_mean': 0.6666666666666666}\n",
      "'https://ai.azure.com/build/evaluation/promptflow_evals_evaluate_model_sampler_model_sampler_modelsampler_9z7resds_20240807_125153_249924?wsid=/subscriptions/2d385bf4-0756-4a76-aa95-28bf9ed3b625/resourceGroups/rg-ninhuai/providers/Microsoft.MachineLearningServices/workspaces/ninhu-3593'\n"
     ]
    }
   ],
   "source": [
    "print(result['metrics'])\n",
    "print(result['studio_url'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
